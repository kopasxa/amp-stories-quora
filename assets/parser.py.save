import re
import time
import config
from bs4 import BeautifulSoup as bs
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from assets.page_builder import PageBuilder

class Parse:
    def __init__(self, url):
        from selenium import webdriver

        self.search_url = url
        #self.options = webdriver.ChromeOptions()
        self.options = webdriver.FirefoxOptions()
        self.articles = []

        self.options.add_argument("--log-level=OFF")
        try:
            self.options.add_argument('--headless') # for firefox on server
        except:
            pass # for chrome of debug mode

        #self.driver = webdriver.Chrome(options=self.options, executable_path="assets/chromedriver.exe")
        self.driver = webdriver.Firefox(options=self.options)

    def search(self, filter):
        self.driver.get(url=self.search_url)

        wait = WebDriverWait(self.driver, 5)
        try:
            wait.until(lambda x: x.find_element(By.CSS_SELECTOR, "div#mainContent"))
        except:
            raise Exception("Timeout error")

        time.sleep(3)
        
        while True:
            if self.driver.find_element(By.CSS_SELECTOR, "#mainContent > div > div > div:nth-child(2) > div:last-child").text == "":
                time.sleep(1)
                self.driver.execute_script("window.scrollTo(0, document.querySelector('#root').scrollHeight);")
            else:
                break

        result = self.get_articles_by_filter(self.driver.page_source, filter)
        self.articles += result
        return result

    def get_articles_by_filter(self, page, filter):
        soup = bs(page, 'html.parser')
        arcticles_list = []
        articles = soup.select('div.qu-borderColor--raised:nth-child(1) div:nth-child(2) div span.q-text a.qu-display--block')

        for idx, article in enumerate(articles):
            arcticles_list.append(article['href'])

        return arcticles_list

    def get_title(self, page):
        soup = bs(page, 'html.parser')
        try:
            title = soup.select_one('#mainContent > div.q-box.qu-borderAll.qu-borderRadius--small.qu-borderColor--raised.qu-boxShadow--small.qu-bg--raised > div > div > div > div.q-text.qu-dynamicFontSize--xlarge.qu-fontWeight--bold.qu-color--gray_dark_dim.qu-passColorToLinks.qu-lineHeight--regular.qu-wordBreak--break-word > span > span > div > div > div > span').get_text()
        except:
            return None

        if len(title) <= 250:
            return title

        return None

    def find_stories(self, page):
        stories = []
        try:
            soup = bs(page, 'html.parser')

            #answer = soup.select_one("#mainContent > div.q-box.qu-borderAll.qu-borderRadius--small.qu-borderColor--raised.qu-boxShadow--small.qu-bg--raised > div > div > div > div.q-text.qu-dynamicFontSize--xlarge.qu-fontWeight--bold.qu-color--gray_dark_dim.qu-passColorToLinks.qu-lineHeight--regular.qu-wordBreak--break-word > span > span > div > div > div > span > span").text
            answers = soup.select("#mainContent > div:nth-child(2) > div > div")
            for answer in answers:
                try:
                    if "dom_annotate_question_answer_item" in answer.select_one("div")['class'][1]:
                        answer = answer.find("div", {"class": "puppeteer_test_answer_content"})

                        if len(answer.text) <= config.limit_of_answers and len(answer.text) > 0:
                            stories.append({'answer': answer.text.capitalize()})
                except:
                    pass

            if len(stories) > 0:
                return stories

            return None

        except Exception as e:
            print(e)
            return None

    def register_sitemap(self, url):
        import xml.etree.ElementTree as ET
        from datetime import datetime

        mytree = ET.parse(f'{config.path_root}/sitemap.xml')
        myroot = mytree.getroot()
        
        el = ET.SubElement(myroot, 'url')
        ET.SubElement(el, 'loc').text = url
        ET.SubElement(el, 'lastmod').text = datetime.strftime(datetime.now(), '%Y-%m-%d')

        ET.register_namespace("", "http://www.sitemaps.org/schemas/sitemap/0.9")

        mytree.write(f'{config.path_root}/sitemap.xml')

    def run_page_builder(self, pages):
        for url in pages:
            self.driver.get(url)
            wait = WebDriverWait(self.driver, 10)
            while True:
                try:
                    wait.until(lambda x: x.find_element(By.CSS_SELECTOR, "div#mainContent"))
                    break
                except:
                    print())
                    time.sleep(60*3)

            title = self.get_title(self.driver.page_source)
            stories = self.find_stories(self.driver.page_source)
            #print(url, stories)

            if title != None and stories != None:
                build = PageBuilder()
                build.set_page_title(title)

                build.build_start_page()
                build.build_page_head()
                build.set_page_link(url)
                build.build_story_start(config.publisher, config.publisher_logo)
                build.build_story(story_type="first_story")

                for story in stories:
                    build.build_story(story['answer'])

                build.build_end_page()

                reg = re.compile('[^0-9\s^a-zA-Z]')
                title = reg.sub('', title)
                path = f'{config.path_to_stories}/{"_".join(title.split(" ")).split(".")[0]}'

                build.build_page(path)

                self.register_sitemap(f'{config.my_domain}{path.split(config.path_root)[1]}.html')

                time.sleep(config.timeout_page_generate)

    def __del__(self):
        self.driver.quit()
